{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b602b9f6-6b2d-40d4-9dfc-b94de4123bef",
   "metadata": {},
   "source": [
    "# Campaign Bried Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b4ea9e-7e52-4547-8948-aa339c6cb69d",
   "metadata": {},
   "source": [
    "In this campaign brief creation, \n",
    "We have 3 Agents\n",
    "1. Supervisor Agent: Manages the entire agents workflow\n",
    "2. Brief Creator Agent: Create the brief according the given prompt\n",
    "3. Summariser Agent: Summarises the entire brief from the previously existing data.\n",
    "\n",
    "and we have 3 Tools:\n",
    "1. extract_placeholders_from_template: Extracts the placeholders from the template\n",
    "2. aggregate_text_files: Stich all the previous campaign brief txt files as a single file and pass it with LLM's.\n",
    "3. populate_word_from_json: Write back the content into the word file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003a40c4-c47c-46f8-9362-fa377340efe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic import for LLM and Langchain Agents\n",
    "import os\n",
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain_community.llms import AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain_openai import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1146c2-58c1-4fae-875b-64864ee24506",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the Azure OpenAI 4o Mini LLM\n",
    "llm = AzureChatOpenAI(model=\"gpt-4o-mini\",\n",
    "    api_version=\"2025-01-01-preview\",\n",
    "    azure_endpoint=\"\",\n",
    "    api_key=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49a2517-db8c-4510-80bf-bfceb56ae531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tool 1: Extracts the placeholders from the templates\n",
    "\n",
    "# ----- START: Code Block for Defining extract_placeholders_tool (Refinement Removed) -----\n",
    "\n",
    "import re # Import regular expressions module\n",
    "import os\n",
    "from typing import List, Dict, Any # Import necessary types\n",
    "# from langchain_core.pydantic_v1 import BaseModel, Field # Using v1 for consistency\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.tools import StructuredTool\n",
    "import docx # Requires: pip install python-docx\n",
    "\n",
    "print(\"--- Defining Word placeholder extraction tool dependencies and function (Refinement Removed) ---\")\n",
    "\n",
    "# --- Pydantic Schema for Tool Arguments ---\n",
    "class ExtractPlaceholdersArgs(BaseModel):\n",
    "    \"\"\"Input schema for the ExtractPlaceholdersTool.\"\"\"\n",
    "    template_path: str = Field(description=\"Path to the Word template (.docx) file containing placeholders like {{PLACEHOLDER_NAME}}.\")\n",
    "\n",
    "# --- Placeholder Refinement Mapping (REMOVED) ---\n",
    "# placeholder_refinement_map: Dict[str, str] = {\n",
    "#     \"{{PLACEHOLDER_ROLES}}\": \"{{PLACEHOLDER_ROLES_AND_RESPONSIBILITIES}}\",\n",
    "# }\n",
    "\n",
    "# --- Core Python Function (Refinement Removed) ---\n",
    "def extract_placeholders_func(template_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    (Refinement Removed) Extracts placeholders (like {{PLACEHOLDER_NAME}})\n",
    "    from a Word document template and returns the list of unique placeholders exactly as found.\n",
    "\n",
    "    Args:\n",
    "        template_path: Path to the .docx template file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing:\n",
    "         - 'extracted_placeholders': A list of unique placeholder strings found (exactly as in template).\n",
    "         - 'status': A success or error message string.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running extract_placeholders_func (Refinement Removed) ---\")\n",
    "    print(f\"Attempting to extract placeholders from template '{template_path}'...\")\n",
    "    absolute_template_path = os.path.abspath(template_path)\n",
    "    print(f\"DEBUG: Absolute Template Path: {absolute_template_path}\")\n",
    "\n",
    "    if not os.path.exists(template_path):\n",
    "        error_msg = f\"Error: Template file not found at '{template_path}'\"\n",
    "        print(error_msg)\n",
    "        return {\"extracted_placeholders\": [], \"status\": error_msg}\n",
    "\n",
    "    try:\n",
    "        doc = docx.Document(template_path)\n",
    "        placeholder_regex = re.compile(r\"\\{\\{\\s*(.*?)\\s*\\}\\}\")\n",
    "        found_placeholders = set()\n",
    "\n",
    "        # --- Function to search within text runs (Refinement Removed) ---\n",
    "        def find_in_runs(runs):\n",
    "             full_text = \"\".join(run.text for run in runs)\n",
    "             matches = placeholder_regex.findall(full_text)\n",
    "             for match_content in matches:\n",
    "                 # CHANGE: Use original placeholder directly, no refinement lookup\n",
    "                 original_placeholder = f\"{{{{{match_content}}}}}\"\n",
    "                 found_placeholders.add(original_placeholder)\n",
    "                 # CHANGE: Simplified print statement\n",
    "                 print(f\"  Found '{original_placeholder}'\")\n",
    "\n",
    "        # --- Search in regular paragraphs ---\n",
    "        print(\"\\nChecking regular paragraphs...\")\n",
    "        for paragraph in doc.paragraphs:\n",
    "            if '{{' in paragraph.text and '}}' in paragraph.text:\n",
    "                 find_in_runs(paragraph.runs)\n",
    "\n",
    "        # --- Search within tables ---\n",
    "        print(\"\\nChecking tables...\")\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                for cell in row.cells:\n",
    "                    for paragraph in cell.paragraphs:\n",
    "                         if '{{' in paragraph.text and '}}' in paragraph.text:\n",
    "                             find_in_runs(paragraph.runs)\n",
    "\n",
    "        if not found_placeholders:\n",
    "            status_msg = f\"No placeholders like {{...}} found in '{template_path}'.\"\n",
    "            print(status_msg)\n",
    "            return {\"extracted_placeholders\": [], \"status\": status_msg}\n",
    "        else:\n",
    "            sorted_placeholders = sorted(list(found_placeholders))\n",
    "            status_msg = f\"Successfully extracted {len(sorted_placeholders)} unique placeholders from '{template_path}'.\"\n",
    "            print(status_msg)\n",
    "            print(f\"Placeholders found: {sorted_placeholders}\")\n",
    "            return {\"extracted_placeholders\": sorted_placeholders, \"status\": status_msg}\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error during placeholder extraction from '{template_path}': {e}\"\n",
    "        print(error_msg)\n",
    "        return {\"extracted_placeholders\": [], \"status\": error_msg}\n",
    "\n",
    "# --- Create the LangChain StructuredTool (Updated Description) ---\n",
    "extract_placeholders_tool = StructuredTool.from_function(\n",
    "    func=extract_placeholders_func,\n",
    "    name=\"extract_placeholders_from_template\",\n",
    "    # CHANGE: Update description to remove mention of refinement\n",
    "    description=\"Reads a Word document (.docx) template, extracts all unique placeholders like {{PLACEHOLDER_NAME}} found within it (in paragraphs and tables), and returns them as a list exactly as they appear in the template.\",\n",
    "    args_schema=ExtractPlaceholdersArgs,\n",
    "    return_direct=False\n",
    ")\n",
    "\n",
    "print(f\"--- Successfully defined tool: {extract_placeholders_tool.name} (Refinement Removed) ---\")\n",
    "\n",
    "# ----- END: Code Block for Defining extract_placeholders_tool (Refinement Removed) -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bf71bf-980f-4490-8633-dbaf79f4a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tool 2: Writing down the newly generated output from agent into word document\n",
    "\n",
    "# ----- START: Modified Code Block for populate_word_from_json_func (More Robust Key Handling) -----\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.tools import StructuredTool\n",
    "import docx\n",
    "\n",
    "print(\"--- Defining Word population tool dependencies and function (Robust Key Handling) ---\")\n",
    "\n",
    "# --- Pydantic Schema for Tool Arguments ---\n",
    "class PopulateWordArgs(BaseModel):\n",
    "    \"\"\"Input schema for the PopulateWordFromJSONTool.\"\"\"\n",
    "    # CHANGE: Reflect more flexible key handling\n",
    "    json_data: Dict[str, Any] = Field(description=\"JSON data (as a Python dictionary). Keys should ideally match the content inside placeholders (e.g., 'PLACEHOLDER_CAMPAIGN_NAME' or just 'CAMPAIGN_NAME'), NOT the full '{{...}}'.\")\n",
    "    template_path: str = Field(description=\"Path to the Word template (.docx) file containing placeholders like {{PLACEHOLDER_KEY}}.\")\n",
    "    output_path: str = Field(description=\"Path where the populated Word document will be saved.\")\n",
    "\n",
    "# --- Core Python Function (Modified for Robust Key Handling) ---\n",
    "def populate_word_from_json_func(json_data: Dict[str, Any], template_path: str, output_path: str) -> str:\n",
    "    \"\"\"\n",
    "    (Robust Keys) Populates a Word document template with data from a JSON object.\n",
    "    Placeholders in the Word document should be like {{PLACEHOLDER_KEY}}.\n",
    "    The keys in the input json_data dictionary can be either 'PLACEHOLDER_KEY' or 'KEY'.\n",
    "    The function will construct the correct '{{PLACEHOLDER_KEY}}' to search for.\n",
    "    Includes debugging.\n",
    "\n",
    "    Args:\n",
    "        json_data: Dictionary containing the data. Keys expected without braces.\n",
    "        template_path: Path to the .docx template file.\n",
    "        output_path: Path to save the populated .docx file.\n",
    "\n",
    "    Returns:\n",
    "        A success or error message string.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running populate_word_from_json_func (Robust Keys) ---\")\n",
    "    print(f\"Attempting to populate template '{template_path}'...\")\n",
    "    if not isinstance(json_data, dict):\n",
    "        msg = f\"ERROR: Input 'json_data' is not a dictionary (received type: {type(json_data)}). Cannot proceed.\"\n",
    "        print(msg)\n",
    "        return msg\n",
    "\n",
    "    print(f\"DEBUG: Received json_data keys: {list(json_data.keys())}\")\n",
    "\n",
    "    # --- Path checking remains the same ---\n",
    "    try:\n",
    "        cwd = os.getcwd()\n",
    "        absolute_template_path = os.path.abspath(template_path)\n",
    "        absolute_output_path = os.path.abspath(output_path)\n",
    "    except Exception as path_e:\n",
    "        print(f\"DEBUG: Error getting path info: {path_e}\")\n",
    "        absolute_template_path = template_path # Fallback\n",
    "        absolute_output_path = output_path # Fallback\n",
    "\n",
    "    print(f\"DEBUG: Absolute Template Path: {absolute_template_path}\")\n",
    "    print(f\"DEBUG: Absolute Output Path: {absolute_output_path}\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(template_path):\n",
    "             print(f\"ERROR: Template file not found at relative path: '{template_path}'\")\n",
    "             if not os.path.exists(absolute_template_path):\n",
    "                 print(f\"ERROR: Template file also not found at absolute path: '{absolute_template_path}'\")\n",
    "             return f\"Error: Template file not found at '{template_path}'\"\n",
    "\n",
    "        doc = docx.Document(template_path)\n",
    "        placeholders_replaced_count = 0\n",
    "        # Use the keys received from the JSON dict\n",
    "        keys_from_json = list(json_data.keys())\n",
    "        keys_successfully_replaced = set() # Track keys that were actually replaced\n",
    "\n",
    "        # --- Helper Function (Modified for Robust Key Handling) ---\n",
    "        def find_and_replace_in_paragraph(paragraph, current_json_keys):\n",
    "            nonlocal placeholders_replaced_count, keys_successfully_replaced\n",
    "            text_replaced_in_para = False\n",
    "\n",
    "            # Iterate through the keys provided in the JSON data\n",
    "            for key_from_json in current_json_keys:\n",
    "                # CHANGE: Construct the full placeholder to search for, adding 'PLACEHOLDER_' if missing\n",
    "                content_key = key_from_json # Start with the key as received\n",
    "                if not content_key.upper().startswith(\"PLACEHOLDER_\"):\n",
    "                    placeholder_to_find = f\"{{{{PLACEHOLDER_{content_key}}}}}\"\n",
    "                else:\n",
    "                    placeholder_to_find = f\"{{{{{content_key}}}}}\"\n",
    "\n",
    "                # Search for the constructed placeholder in the paragraph text\n",
    "                if placeholder_to_find in paragraph.text:\n",
    "                    text_to_insert = str(json_data[key_from_json])\n",
    "                    # Replace using runs for robustness\n",
    "                    inline = paragraph.runs\n",
    "                    for i in range(len(inline)):\n",
    "                        if placeholder_to_find in inline[i].text:\n",
    "                            text = inline[i].text.replace(placeholder_to_find, text_to_insert)\n",
    "                            inline[i].text = text\n",
    "                            print(f\"  Replaced '{placeholder_to_find}' using key '{key_from_json}' in paragraph starting: '{paragraph.text[:50]}...'\")\n",
    "                            keys_successfully_replaced.add(key_from_json) # Track the key from JSON that was used\n",
    "                            text_replaced_in_para = True\n",
    "                            # Assuming one replacement per key per paragraph pass for simplicity\n",
    "                            # If multiple instances of the same placeholder exist, this might need adjustment\n",
    "\n",
    "            return text_replaced_in_para\n",
    "\n",
    "        # --- Search logic remains largely the same ---\n",
    "        print(\"\\nChecking regular paragraphs...\")\n",
    "        remaining_keys = list(keys_from_json) # Start with all keys\n",
    "        find_and_replace_in_paragraph(doc.paragraphs[0], remaining_keys) # Example: Process first paragraph\n",
    "\n",
    "        for paragraph in doc.paragraphs:\n",
    "           find_and_replace_in_paragraph(paragraph, remaining_keys)\n",
    "\n",
    "\n",
    "        print(\"\\nChecking tables...\")\n",
    "        if remaining_keys: # Only check tables if keys might still need replacing\n",
    "            for table_idx, table in enumerate(doc.tables):\n",
    "                for row_idx, row in enumerate(table.rows):\n",
    "                    for cell_idx, cell in enumerate(row.cells):\n",
    "                        for para_idx, paragraph in enumerate(cell.paragraphs):\n",
    "                             find_and_replace_in_paragraph(paragraph, remaining_keys)\n",
    "\n",
    "        # Calculate counts based on keys successfully replaced\n",
    "        placeholders_replaced_count = len(keys_successfully_replaced)\n",
    "        keys_not_found_in_template = [k for k in keys_from_json if k not in keys_successfully_replaced]\n",
    "\n",
    "        if keys_not_found_in_template:\n",
    "            print(f\"\\nWARNING: Values for these JSON keys were provided, but corresponding '{{{{PLACEHOLDER_...}}}}' placeholders were NOT found in the template: {keys_not_found_in_template}\")\n",
    "        else:\n",
    "            print(\"\\nAll provided JSON keys corresponded to placeholders found and replaced.\")\n",
    "        print(f\"Total unique placeholders found and replaced: {placeholders_replaced_count} out of {len(keys_from_json)} provided JSON keys.\")\n",
    "\n",
    "        # --- Directory creation and saving logic remains the same ---\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "             try:\n",
    "                os.makedirs(output_dir)\n",
    "                print(f\"Created output directory: {output_dir}\")\n",
    "             except OSError as dir_e:\n",
    "                 print(f\"ERROR creating output directory '{output_dir}': {dir_e}\")\n",
    "                 return f\"Error creating output directory: {dir_e}\"\n",
    "\n",
    "        print(f\"\\nAttempting to save populated document to: '{output_path}' (Absolute: '{absolute_output_path}')\")\n",
    "        try:\n",
    "            doc.save(output_path)\n",
    "            print(f\"Successfully called doc.save() for: {output_path}\")\n",
    "            if not os.path.exists(output_path):\n",
    "                 print(f\"CRITICAL WARNING: File NOT found at '{output_path}' immediately after save call!\")\n",
    "                 if not os.path.exists(absolute_output_path):\n",
    "                     print(f\"CRITICAL WARNING: File also NOT found at absolute path '{absolute_output_path}'!\")\n",
    "                 return f\"Error: File saving failed silently for {output_path}\"\n",
    "            else:\n",
    "                print(f\"File confirmed to exist at '{output_path}' after saving.\")\n",
    "                return f\"Successfully populated template and saved to '{output_path}'\"\n",
    "        except Exception as e_save:\n",
    "            print(f\"ERROR during doc.save() or file check: {e_save}\")\n",
    "            if isinstance(e_save, PermissionError):\n",
    "                 print(f\"Hint: Check write permissions for the directory '{os.path.dirname(absolute_output_path)}'\")\n",
    "                 print(f\"Hint: Ensure the file '{absolute_output_path}' is not already open in another application.\")\n",
    "            return f\"Error during file save operation: {e_save}\"\n",
    "\n",
    "    except FileNotFoundError:\n",
    "         return f\"Error: Template file not found at '{template_path}' during processing.\"\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Word processing: {e}\")\n",
    "        return f\"Error populating Word template: {e}\"\n",
    "\n",
    "\n",
    "# --- Create the LangChain StructuredTool (Update description) ---\n",
    "populate_word_tool = StructuredTool.from_function(\n",
    "    func=populate_word_from_json_func,\n",
    "    name=\"populate_word_from_json\",\n",
    "    # CHANGE: Update description to reflect robust key handling\n",
    "    description=\"Populates a Word document (.docx) template (placeholders like {{PLACEHOLDER_KEY}}) using data from a JSON object. The KEYS in the input 'json_data' dictionary should match the content inside the braces, ideally including the 'PLACEHOLDER_' prefix (e.g., 'PLACEHOLDER_KEY' or 'KEY'), but NOT the full '{{...}}'.\",\n",
    "    args_schema=PopulateWordArgs,\n",
    ")\n",
    "\n",
    "print(f\"--- Successfully defined tool: {populate_word_tool.name} (Robust Key Handling) ---\")\n",
    "\n",
    "# ----- END: Modified Code Block for populate_word_from_json_func (Robust Key Handling) -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db253a7-0ee9-4c85-a29f-af57b2ed1967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tool 3: Aggregating the previous campaign brief txt files into one together as input for summariser agent\n",
    "# import os\n",
    "# # from langchain.agents import Tool # Original import (can be removed if not used elsewhere)\n",
    "# from langchain.tools import StructuredTool # <--- CHANGE: Import StructuredTool\n",
    "\n",
    "# def aggregate_text_files_tool_func() -> str: # Function definition remains the same\n",
    "#     \"\"\"\n",
    "#     Tool to aggregate content from all .txt files in the './Data/' directory.\n",
    "#     \"\"\"\n",
    "#     directory_path = \"./Data/\" # Hardcoded default directory\n",
    "#     print(f\"aggregate_text_files_tool_func is trying to access directory: {directory_path}\") # DEBUG PRINT\n",
    "#     aggregated_text = \"\"\n",
    "#     try:\n",
    "#         print(f\"Listing files in directory: {directory_path}\") # DEBUG PRINT\n",
    "#         filenames = os.listdir(directory_path)\n",
    "#         print(f\"Files found: {filenames}\") # DEBUG PRINT\n",
    "#         for filename in filenames:\n",
    "#             if filename.endswith(\".txt\"):\n",
    "#                 filepath = os.path.join(directory_path, filename)\n",
    "#                 print(f\"Processing file: {filepath}\") # DEBUG PRINT\n",
    "#                 with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "#                     aggregated_text += f.read() + \"\\n\\n\"\n",
    "#         if not aggregated_text:\n",
    "#             return \"No .txt files found in the directory.\"\n",
    "#         print (\"Returning Aggregated Text\")\n",
    "#         return aggregated_text\n",
    "#     except FileNotFoundError:\n",
    "#         return f\"Directory '{directory_path}' not found.\"\n",
    "\n",
    "# # Use StructuredTool.from_function\n",
    "# aggregate_data_tool = StructuredTool.from_function( # <--- CHANGE: Use StructuredTool\n",
    "#     func=aggregate_text_files_tool_func,\n",
    "#     name=\"aggregate_text_files\",\n",
    "#     description=\"Useful for aggregating content from previous campaign data files in a directory.\",\n",
    "#     args_schema=None # Explicitly set args_schema to None for zero-input tool still works here\n",
    "# )\n",
    "\n",
    "# # tools_for_supervisor = [aggregate_data_tool, populate_word_tool]\n",
    "# # print(f\"Tools available for supervisor: {[tool.name for tool in tools_for_supervisor]}\") # Optional: Confirmation print\n",
    "# # ----- END CELL 1 (Modified) -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb3b317c-0476-4b55-9f77-8d1685a60e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Using cached chromadb-1.0.5-cp39-abi3-win_amd64.whl.metadata (7.0 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Using cached build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from chromadb) (2.11.2)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
      "  Using cached chroma_hnswlib-0.7.6.tar.gz (32 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting fastapi==0.115.9 (from chromadb)\n",
      "  Using cached fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from chromadb) (2.2.4)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Using cached posthog-3.25.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from chromadb) (4.13.1)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Using cached onnxruntime-1.21.1-cp313-cp313-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_api-1.32.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_sdk-1.32.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Using cached pypika-0.48.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Using cached grpcio-1.71.0-cp313-cp313-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Using cached bcrypt-4.3.0-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Using cached typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Using cached kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Using cached mmh3-5.1.0-cp313-cp313-win_amd64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from chromadb) (3.10.16)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Collecting rich>=10.11.0 (from chromadb)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from chromadb) (4.23.0)\n",
      "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\n",
      "  Using cached starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: anyio in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached google_auth-2.39.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached protobuf-6.30.2-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting importlib-metadata<8.7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_proto-1.32.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached protobuf-5.29.4-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_instrumentation-0.53b1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_util_http-0.53b1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached wrapt-1.17.2-cp313-cp313-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers>=0.13.2->chromadb)\n",
      "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached httptools-0.6.4-cp313-cp313-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached watchfiles-1.0.5-cp313-cp313-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached websockets-15.0.1-cp313-cp313-win_amd64.whl.metadata (7.0 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting filelock (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb)\n",
      "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\poc\\agentic ai\\campaignbriefcreation\\cbrief\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Using cached chromadb-1.0.5-cp39-abi3-win_amd64.whl (18.2 MB)\n",
      "Using cached fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
      "Using cached bcrypt-4.3.0-cp39-abi3-win_amd64.whl (152 kB)\n",
      "Using cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Using cached grpcio-1.71.0-cp313-cp313-win_amd64.whl (4.3 MB)\n",
      "Using cached kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
      "Using cached mmh3-5.1.0-cp313-cp313-win_amd64.whl (41 kB)\n",
      "Using cached onnxruntime-1.21.1-cp313-cp313-win_amd64.whl (12.3 MB)\n",
      "Using cached opentelemetry_api-1.32.1-py3-none-any.whl (65 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_proto-1.32.1-py3-none-any.whl (55 kB)\n",
      "Using cached opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl (12 kB)\n",
      "Using cached opentelemetry_instrumentation-0.53b1-py3-none-any.whl (30 kB)\n",
      "Using cached opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl (16 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl (188 kB)\n",
      "Using cached opentelemetry_util_http-0.53b1-py3-none-any.whl (7.3 kB)\n",
      "Using cached opentelemetry_sdk-1.32.1-py3-none-any.whl (118 kB)\n",
      "Using cached posthog-3.25.0-py2.py3-none-any.whl (89 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Using cached uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Using cached durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Using cached google_auth-2.39.0-py2.py3-none-any.whl (212 kB)\n",
      "Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Using cached httptools-0.6.4-cp313-cp313-win_amd64.whl (87 kB)\n",
      "Using cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Using cached importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached protobuf-5.29.4-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached starlette-0.45.3-py3-none-any.whl (71 kB)\n",
      "Using cached watchfiles-1.0.5-cp313-cp313-win_amd64.whl (291 kB)\n",
      "Using cached websockets-15.0.1-cp313-cp313-win_amd64.whl (176 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached wrapt-1.17.2-cp313-cp313-win_amd64.whl (38 kB)\n",
      "Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Building wheels for collected packages: chroma-hnswlib\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml): started\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml): finished with status 'error'\n",
      "Failed to build chroma-hnswlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for chroma-hnswlib (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [5 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_ext\n",
      "  building 'hnswlib' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for chroma-hnswlib\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (chroma-hnswlib)\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a273868-b023-4df2-96b6-977705d8ec73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from: ./Data/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 831.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 documents.\n",
      "Splitting documents into chunks...\n",
      "Split into 22 text chunks.\n",
      "Initializing Azure OpenAI embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model initialized.\n",
      "Creating/updating vector store at: ./vector_store_db\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import chromadb python package. Please install it with `pip install chromadb`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\POC\\Agentic Ai\\CampaignBriefCreation\\cbrief\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:83\u001b[39m, in \u001b[36mChroma.__init__\u001b[39m\u001b[34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'chromadb'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError: Missing one or more required Azure environment variables: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequired_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[43mcreate_or_update_vector_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mcreate_or_update_vector_store\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEmbedding model initialized.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreating/updating vector store at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPERSIST_DIRECTORY\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m vectordb = \u001b[43mChroma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPERSIST_DIRECTORY\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m vectordb.persist()\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mVector store created/updated and persisted successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\POC\\Agentic Ai\\CampaignBriefCreation\\cbrief\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:887\u001b[39m, in \u001b[36mChroma.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    885\u001b[39m texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    886\u001b[39m metadatas = [doc.metadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\POC\\Agentic Ai\\CampaignBriefCreation\\cbrief\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:817\u001b[39m, in \u001b[36mChroma.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    784\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_texts\u001b[39m(\n\u001b[32m    786\u001b[39m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[32m   (...)\u001b[39m\u001b[32m    796\u001b[39m     **kwargs: Any,\n\u001b[32m    797\u001b[39m ) -> Chroma:\n\u001b[32m    798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[32m    799\u001b[39m \n\u001b[32m    800\u001b[39m \u001b[33;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    815\u001b[39m \u001b[33;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[32m    816\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m817\u001b[39m     chroma_collection = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    819\u001b[39m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    821\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    827\u001b[39m         ids = [\u001b[38;5;28mstr\u001b[39m(uuid.uuid4()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\POC\\Agentic Ai\\CampaignBriefCreation\\cbrief\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:221\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    219\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    220\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\POC\\Agentic Ai\\CampaignBriefCreation\\cbrief\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:86\u001b[39m, in \u001b[36mChroma.__init__\u001b[39m\u001b[34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[39m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     87\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import chromadb python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     88\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install chromadb`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     89\u001b[39m     )\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     92\u001b[39m     \u001b[38;5;28mself\u001b[39m._client_settings = client_settings\n",
      "\u001b[31mImportError\u001b[39m: Could not import chromadb python package. Please install it with `pip install chromadb`."
     ]
    }
   ],
   "source": [
    "# build_vector_store.py (Azure Version)\n",
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings,AzureOpenAIEmbeddings # Handles Azure via env vars\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # Load environment variables from .env\n",
    "\n",
    "# --- Configuration ---\n",
    "SOURCE_DIRECTORY = \"./Data/\"\n",
    "PERSIST_DIRECTORY = \"./vector_store_db\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 150\n",
    "# --- Azure environment variables are expected to be loaded ---\n",
    "\n",
    "def create_or_update_vector_store():\n",
    "    \"\"\"\n",
    "    Loads .txt files, splits them, creates embeddings using Azure OpenAI,\n",
    "    and stores them in Chroma.\n",
    "    \"\"\"\n",
    "    print(f\"Loading documents from: {SOURCE_DIRECTORY}\")\n",
    "    loader = DirectoryLoader(\n",
    "        SOURCE_DIRECTORY, glob=\"**/*.txt\", loader_cls=TextLoader,\n",
    "        loader_kwargs={'encoding': 'utf-8'}, show_progress=True, use_multithreading=True\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    if not documents:\n",
    "        print(\"No .txt documents found. Exiting.\")\n",
    "        return\n",
    "    print(f\"Loaded {len(documents)} documents.\")\n",
    "\n",
    "    print(\"Splitting documents into chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    print(f\"Split into {len(texts)} text chunks.\")\n",
    "\n",
    "    print(\"Initializing Azure OpenAI embedding model...\")\n",
    "    # OpenAIEmbeddings automatically uses Azure variables if OPENAI_API_TYPE=azure\n",
    "    # It reads AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME for the deployment\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        # deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"), # Usually picked up automatically\n",
    "        model=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"), # Often need to specify model name same as deployment for Azure\n",
    "        chunk_size=1 # Recommended for Azure embeddings\n",
    "    )\n",
    "    print(\"Embedding model initialized.\")\n",
    "\n",
    "    print(f\"Creating/updating vector store at: {PERSIST_DIRECTORY}\")\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=texts,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=PERSIST_DIRECTORY\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    print(\"Vector store created/updated and persisted successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure .env is loaded before anything else if running directly\n",
    "    load_dotenv()\n",
    "    if not os.path.exists(PERSIST_DIRECTORY):\n",
    "        os.makedirs(PERSIST_DIRECTORY)\n",
    "    # Ensure all required env vars are present before running\n",
    "    required_vars = ['OPENAI_API_TYPE', 'OPENAI_API_VERSION', 'AZURE_OPENAI_ENDPOINT', 'OPENAI_API_KEY', 'AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME']\n",
    "    if not all(os.getenv(var) for var in required_vars):\n",
    "        print(f\"Error: Missing one or more required Azure environment variables: {required_vars}\")\n",
    "    else:\n",
    "        create_or_update_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030c1f7c-37d5-43e2-9013-60b35cd00913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the tools used by the Supervisor Agent\n",
    "tools_for_supervisor = [extract_placeholders_tool,aggregate_data_tool, populate_word_tool]\n",
    "print(f\"Tools available for supervisor: {[tool.name for tool in tools_for_supervisor]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26da765-fd1f-49e3-a24a-599e61930d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 1: Summariser Agent: Summarises the old campaign briefs shared by aggregate_data_tool and parse to Brief Generator Agent to create new brief.\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import SystemMessage # Import SystemMessage\n",
    "\n",
    "# 2. Define Summarizer Prompt - USING ChatPromptTemplate and MessagesPlaceholder\n",
    "# Define the system message part separately (clearer instructions for the LLM)\n",
    "summarizer_system_message = \"\"\"You are a Campaign Data Summarizer Agent.\n",
    "Your input is a list of messages containing the conversation history.\n",
    "\n",
    "Your specific task is:\n",
    "1.  Examine the provided message history ('messages').\n",
    "2.  **Locate the ToolMessage containing the output from the 'extract_placeholders_from_template' tool.** Extract the list of required `extracted_placeholders` from its content. Note these required fields.\n",
    "3.  **Locate the most recent ToolMessage containing the output from the 'aggregate_text_files' tool.** Extract the raw aggregated text content from it.\n",
    "4.  **Summarize the extracted raw text content.**\n",
    "\n",
    "**When summarizing, focus on extracting key information generally relevant for creating new campaign briefs, such as:**\n",
    "    - Target audience insights\n",
    "    - Past campaign objectives and strategies\n",
    "    - Key learnings (successes/failures)\n",
    "    - Performance metrics/results\n",
    "    - Observed trends/patterns and others\n",
    "\n",
    "**IMPORTANT:** While extracting the above, **pay special attention and prioritize information that seems directly relevant to the topics indicated by the `extracted_placeholders` list** you noted in step 2. Your goal is to provide a summary that is both generally informative AND maximally useful for filling the specific fields required by the template.\n",
    "\n",
    "Your final output should be ONLY the concise, informative summary containing the prioritized, relevant information.\"\"\"\n",
    "\n",
    "# Create the ChatPromptTemplate using MessagesPlaceholder\n",
    "summarizer_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=summarizer_system_message),\n",
    "    MessagesPlaceholder(variable_name=\"messages\") # CRITICAL: Use MessagesPlaceholder\n",
    "])\n",
    "\n",
    "# 3. Create the Summarizer Agent - Using the new prompt structure\n",
    "summarizer_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[], # No tools for the summarizer itself\n",
    "    prompt=summarizer_prompt_template, # Use the MODIFIED ChatPromptTemplate\n",
    "    name=\"summarizer_agent\"\n",
    "    # create_react_agent is designed to work with MessagesPlaceholder and should\n",
    "    # automatically pass the 'messages' list from the input state.\n",
    ")\n",
    "\n",
    "# 4. Example Usage and Testing (Optional for now)\n",
    "# ...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Summarizer Agent defined successfully with ChatPromptTemplate!\") # Confirmation message\n",
    "# ----- END CELL 2 (Modified Again) -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76928374-4da0-4301-9461-f11cdc5e0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 3: Brief Generator Agent: This Agent generates the new brief acc to the input prompt by user and keeping the context of previous campaign with the help of summariser agent and generated the JSON output as a new brief.\n",
    "from langchain_openai import ChatOpenAI # Updated import if using newer langchain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# 2. Define Brief Generator Prompt - ENHANCED FOR COMPLETENESS\n",
    "#    Focus is on ensuring ALL placeholders from the tool output are addressed.\n",
    "brief_generator_system_message = \"\"\"You are a meticulous Campaign Brief Generator Agent.\n",
    "Your primary goal is to create a comprehensive draft campaign brief by synthesizing information and ensuring ALL required sections are included.\n",
    "\n",
    "Your input is a list of messages representing the conversation history.\n",
    "\n",
    "**Your Task Breakdown:**\n",
    "\n",
    "1.  **Identify Inputs:** Carefully examine the provided message history ('messages') to locate:\n",
    "    *   The initial `HumanMessage` containing the user's original request and core requirements.\n",
    "    *   The most recent `AIMessage` (likely from a 'summarizer_agent') containing relevant summarized data.\n",
    "    *   The `ToolMessage` containing the output from the 'extract_placeholders_from_template' tool. **This message contains the critical list of `extracted_placeholders`.**\n",
    "\n",
    "2.  **Confirm Placeholders:** Extract the exact list of `extracted_placeholders` from the `ToolMessage`. Let's call this the `REQUIRED_SECTIONS_LIST`.\n",
    "\n",
    "3.  **Generate Content for ALL Placeholders:** This is the most critical step. Iterate through **every single item** in the `REQUIRED_SECTIONS_LIST`. For each placeholder string:\n",
    "    *   Synthesize relevant information *specifically for that placeholder's topic* using the user's request (from step 1a) and the focused summary (from step 1b).\n",
    "    *   Generate clear and concise content that directly addresses the placeholder's purpose (e.g., for `{{PLACEHOLDER_OBJECTIVES}}`, generate the campaign objectives; for `{{PLACEHOLDER_CORE_MESSAGE}}`, generate the core message).\n",
    "    *   **Handling Missing Information:** If the available inputs do not contain explicit information for a specific placeholder, you MUST still include the section. Use your knowledge to infer a reasonable starting point OR clearly state 'To be determined based on [relevant factor]' or 'N/A - Requires further input'. **Crucially, DO NOT OMIT THE SECTION/PLACEHOLDER itself under any circumstances.**\n",
    "4.  **Structure and Combine Output:** Assemble the generated content for all placeholders into a single, coherent campaign brief text.\n",
    "    *   **Generate Content Under Headings:** Internally or in your text output, use clear headings corresponding to the placeholders to ensure you cover everything (e.g., \"OBJECTIVES:\", \"CORE_MESSAGE:\", \"BUDGET:\", \"ASSETS:\", etc.).\n",
    "    *   **CRITICAL - Key Naming for Downstream Use:** Ensure that when this information is eventually structured (e.g., into JSON), the keys used **MUST EXACTLY MATCH** the placeholder names without the brackets and prefix.\n",
    "    *   **Provide Explicit Mapping (Example within Prompt):**\n",
    "        *   Content for `{{PLACEHOLDER_CAMPAIGN_NAME}}` must correspond to the key `CAMPAIGN_NAME`.\n",
    "        *   Content for `{{PLACEHOLDER_CAMPAIGN_TYPE}}` must correspond to the key `CAMPAIGN_TYPE`.\n",
    "        *   Content for `{{PLACEHOLDER_OBJECTIVES}}` must correspond to the key `OBJECTIVES`.\n",
    "        *   Content for `{{PLACEHOLDER_AUDIENCE}}` must correspond to the key `AUDIENCE`.\n",
    "        *   Content for `{{PLACEHOLDER_CHANNELS}}` must correspond to the key `CHANNELS`.\n",
    "        *   Content for `{{PLACEHOLDER_DURATION}}` must correspond to the key `DURATION`.\n",
    "        *   Content for `{{PLACEHOLDER_BUDGET}}` must correspond to the key `BUDGET`. (NOT 'BUDGET ALLOCATION')\n",
    "        *   Content for `{{PLACEHOLDER_CORE_MESSAGE}}` must correspond to the key `CORE_MESSAGE`.\n",
    "        *   Content for `{{PLACEHOLDER_ASSETS}}` must correspond to the key `ASSETS`. (NOT 'ASSETS REQUIRED')\n",
    "        *   Content for `{{PLACEHOLDER_COMPLIANCE}}` must correspond to the key `COMPLIANCE`.\n",
    "        *   Content for `{{PLACEHOLDER_TECHNICAL}}` must correspond to the key `TECHNICAL`.\n",
    "        *   Content for `{{PLACEHOLDER_MEASUREMENT}}` must correspond to the key `MEASUREMENT`. (NOT 'MEASUREMENT & REPORTING')\n",
    "        *   Content for `{{PLACEHOLDER_INSIGHTS}}` must correspond to the key `INSIGHTS`.\n",
    "        *   Content for `{{PLACEHOLDER_ROLES}}` must correspond to the key `ROLES`. (NOT 'ROLES & RESPONSIBILITIES')\n",
    "    *   **Final Check:** Before outputting, verify that you have included content for **every single placeholder** from the `REQUIRED_SECTIONS_LIST` and that the structure facilitates the correct key mapping described above.\n",
    "\n",
    "5.  **Final Output:** Your final output MUST be ONLY the generated campaign brief text, structured clearly section by section, ready for conversion into a data structure using the precise keys listed above.\n",
    "\"\"\"\n",
    "# Create the ChatPromptTemplate using MessagesPlaceholder\n",
    "brief_generator_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=brief_generator_system_message),\n",
    "    MessagesPlaceholder(variable_name=\"messages\") # Input messages history\n",
    "])\n",
    "\n",
    "# 3. Create the Brief Generator Agent - Using the enhanced prompt\n",
    "brief_generator_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[], # Generator doesn't call other tools, it synthesizes\n",
    "    prompt=brief_generator_prompt_template, # Use the ENHANCED ChatPromptTemplate\n",
    "    name=\"brief_generator_agent\"\n",
    "    # create_react_agent maps the 'messages' list from the graph state automatically\n",
    ")\n",
    "\n",
    "# 4. Example Usage and Testing (Conceptual - depends on your LangGraph setup)\n",
    "#    This agent would typically be a node in your LangGraph.\n",
    "#    Input state would contain the 'messages' list including user request, summary, and placeholder tool output.\n",
    "#    Output state would contain the generated brief text in the 'messages' list as an AIMessage.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This part is just for confirmation during development/testing setup\n",
    "    print(\"Brief Generator Agent defined with ENHANCED prompt!\")\n",
    "    print(\"\\n--- Sample Prompt Structure ---\")\n",
    "    # You can optionally print the template to review structure (for debugging)\n",
    "    # print(brief_generator_prompt_template.pretty_print())\n",
    "    print(\"System Message Length:\", len(brief_generator_system_message)) # Check length if concerned about token limits\n",
    "# ----- END CELL 3 (Modified & Enhanced Prompt) -----"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a8c1686-2859-4d31-b315-27715a4db02d",
   "metadata": {},
   "source": [
    "# Langchain Supervisor Agent Library\n",
    "!pip install langgraph-supervisor --quiet"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d933588e-69ea-4d89-9f41-15a3dd7bc981",
   "metadata": {},
   "source": [
    "pip install --upgrade langgraph"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17b35ac2-bd1f-433d-8b22-091d397aeab5",
   "metadata": {},
   "source": [
    "pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea4322-a4cf-4497-91a9-960927100e91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 7. Define Supervisor Agent and Workflow using create_supervisor\n",
    "from langgraph_supervisor import create_supervisor\n",
    "supervisor_prompt = (\"\"\"You are a Campaign Brief Supervisor Agent. Your job is to manage specialized agents and tools to:\n",
    "1. Identify required information fields (placeholders) from a Word template.\n",
    "2. Gather background data.\n",
    "3. Generate a new campaign brief based on user requirements and background data, structured according to the identified fields.\n",
    "4. Populate the Word template with the generated brief content.\n",
    "\n",
    "You have access to the following tools:\n",
    "\n",
    "1. extract_placeholders_from_template: Reads a Word template (.docx), extracts all unique placeholders like {{PLACEHOLDER_NAME}}, and returns them as a list exactly as they appear. Requires 'template_path'. Use './Data/CampaignBriefCreationTemplate.docx'.\n",
    "2. aggregate_text_files: Aggregates content from previous campaign data files in './Data/'. Does NOT require input arguments.\n",
    "3. populate_word_from_json: Populates a Word template (.docx) using data from a JSON object (dictionary). Requires 'json_data', 'template_path', and 'output_path'. IMPORTANT: The keys in the 'json_data' dictionary for this tool should be the names INSIDE the template braces (e.g., 'PLACEHOLDER_CAMPAIGN_NAME').\n",
    "\n",
    "You also have access to the following agents (interact via message history):\n",
    "\n",
    "1. summarizer_agent: Summarizes previous campaign data. It uses the placeholder list identified earlier to focus its summary on relevant information.\n",
    "2. brief_generator_agent: Generates a new campaign brief. It MUST use the placeholder list identified earlier as the required structure, generating content for each field based on the user prompt and summary.\n",
    "\n",
    "Your workflow MUST be executed in the following precise steps:\n",
    "\n",
    "Step 1: Identify Required Fields. Call the 'extract_placeholders_from_template' tool using './Data/CampaignBriefCreationTemplate.docx'. Remember the list of full placeholders (e.g., '{{PLACEHOLDER_NAME}}') returned.\n",
    "\n",
    "Step 2: Gather Background Data. Call the 'aggregate_text_files' tool (no arguments).\n",
    "\n",
    "Step 3: Summarize Background Data. Delegate the task to 'summarizer_agent'. **It will use the placeholder list found in the history to create a focused summary relevant to the required fields and return that summary.**\n",
    "\n",
    "Step 4: Generate New Campaign Brief. Delegate the task to 'brief_generator_agent'. **It MUST use the placeholder list from the history as the target structure, generating content for each required field based on the user prompt and summary, and return the complete brief text. Make sure to generate content for all the elements in the list**\n",
    "\n",
    "Step 5: Extract Generated Brief. Identify the complete text content of the campaign brief generated by 'brief_generator_agent' in the most recent AIMessage.\n",
    "\n",
    "Step 6: Prepare Data for Word Template (Crucial Reasoning Step). Now, you MUST perform the following actions carefully:\n",
    "    a. **Reference Placeholders:** Recall the list of full placeholders (e.g., '{{PLACEHOLDER_NAME}}') extracted in Step 1.\n",
    "    b. **Parse Generated Brief:** Take the complete campaign brief text extracted in Step 5.\n",
    "    c. **Construct JSON:** Analyze the brief text and CONSTRUCT a JSON object (Python dictionary). The keys MUST EXACTLY match the **content INSIDE** the placeholders from Step 6a (e.g., for '{{PLACEHOLDER_NAME}}', the key is 'PLACEHOLDER_NAME'). The values should be the corresponding text extracted or synthesized from the generated brief for each section. Ensure all placeholder *contents* from Step 6a are present as keys.\n",
    "    d. **Validate JSON:** Ensure a valid Python dictionary.\n",
    "\n",
    "Step 7: Save to Word Document. Call the 'populate_word_from_json' tool. Provide arguments EXACTLY:\n",
    "    - `json_data`: The JSON object from Step 6c.\n",
    "    - `template_path`: './Data/CampaignBriefCreationTemplate.docx'\n",
    "    - `output_path`: './output/Final_Campaign_Brief.docx'\n",
    "\n",
    "Step 8: Final Confirmation. Output the confirmation message returned by the 'populate_word_from_json' tool.\n",
    "\n",
    "Begin the process following Step 1.\n",
    "\"\"\")\n",
    "\n",
    "# Recreate the supervisor workflow with the updated prompt\n",
    "supervisor_workflow = create_supervisor(\n",
    "    [summarizer_agent, brief_generator_agent], # List of agents supervised\n",
    "    model = llm, # LLM for the supervisor agent\n",
    "    tools = tools_for_supervisor, # Pass the tool to the supervisor\n",
    "    prompt = supervisor_prompt\n",
    ")\n",
    "\n",
    "# Recompile and Rerun the app (Cell 5 code)\n",
    "app = supervisor_workflow.compile()\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Please enter the requirements for the new campaign brief.\")\n",
    "print(\"Example: Create a new campaign brief for a winter holiday season promotion targeting families...\")\n",
    "print(\"Paste your brief details below and press Enter when done:\")\n",
    "print(\"---\")\n",
    "new_brief_prompt = input() # Read input from the console\n",
    "\n",
    "# Optional: Add a check for empty input\n",
    "if not new_brief_prompt.strip():\n",
    "    print(\"No input received. Exiting.\")\n",
    "    exit() # Or provide a default prompt, or ask again\n",
    "\n",
    "# --- MODIFICATION END ---\n",
    "\n",
    "\n",
    "# Use the user-provided input in the invoke call\n",
    "print(\"\\n--- Invoking Workflow with User Prompt ---\")\n",
    "result = app.invoke({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"User's New Campaign Brief Prompt: {new_brief_prompt}\" # Use the variable holding user input\n",
    "            }\n",
    "        ]\n",
    "    },{\"recursion_limit\": 100}) # Set an appropriate recursion limit\n",
    "\n",
    "# Print the final results\n",
    "print(\"\\n--- Workflow Final Output ---\")\n",
    "for m in result['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b16bf5d-735a-42fa-8e94-c55cfb624464",
   "metadata": {},
   "source": [
    "#Approach 2\n",
    "\n",
    "# 7. Define Supervisor Agent and Workflow using create_supervisor\n",
    "\n",
    "# REVISED SUPERVISOR PROMPT\n",
    "supervisor_prompt = (\"\"\"You are a Campaign Brief Supervisor Agent. Your job is to manage specialized agents and tools to generate and populate a campaign brief based on a core concept AND specific user-provided details.\n",
    "\n",
    "**Core Campaign Concept (Provided in Initial User Message):** Focuses on the EcoSmart Thermos for Fall/Winter, targeting eco-conscious individuals, with predefined objectives and core messaging elements.\n",
    "\n",
    "**Specific User Details (Also in Initial User Message):** Includes REQUIRED dynamic information like Campaign Name, Budget, Duration, and potentially Roles.\n",
    "\n",
    "**Your Workflow MUST Incorporate Both Concept & Specifics:**\n",
    "\n",
    "1.  **Identify Placeholders:** Call 'extract_placeholders_from_template' for './Data/CampaignBriefCreationTemplate.docx'. Remember the full list (e.g., '{{PLACEHOLDER_NAME}}').\n",
    "2.  **Gather Background Data:** Call 'aggregate_text_files'.\n",
    "3.  **Summarize Background:** Delegate to 'summarizer_agent'. It uses the placeholder list to focus the summary.\n",
    "4.  **Generate New Campaign Brief:** Delegate to 'brief_generator_agent'. **CRUCIAL:** Instruct it to generate content for ALL placeholders identified in Step 1. It MUST synthesize information from:\n",
    "    *   The Core Campaign Concept (Thermos, Fall/Winter, eco-focus, objectives, message basis).\n",
    "    *   The Specific User Details (Exact Campaign Name, Budget figure, Duration dates, Roles provided).\n",
    "    *   The focused summary from the 'summarizer_agent'.\n",
    "    Ensure the generated brief text explicitly includes the user-provided Campaign Name, Budget, Duration, and Roles in the relevant sections.\n",
    "5.  **Extract Generated Brief:** Get the complete text from 'brief_generator_agent'.\n",
    "6.  **Prepare Data for Word Template (Crucial Reasoning Step):**\n",
    "    a. **Reference Placeholders:** Recall the list from Step 1.\n",
    "    b. **Parse Generated Brief:** Take the text from Step 5.\n",
    "    c. **Construct JSON:** Create a JSON object (Python dictionary). Keys MUST EXACTLY match the content INSIDE the placeholders (e.g., 'PLACEHOLDER_NAME'). Values MUST be the corresponding text from the generated brief. **PAY SPECIAL ATTENTION** to correctly extracting the **specific user-provided values** for 'CAMPAIGN_NAME', 'BUDGET', 'DURATION', and 'ROLES' from the generated text and mapping them to their respective keys. Ensure all placeholder contents from Step 6a are present as keys.\n",
    "    d. **Validate JSON.**\n",
    "7.  **Save to Word:** Call 'populate_word_from_json' with the JSON from Step 6c, template path './Data/CampaignBriefCreationTemplate.docx', and output path './output/Final_Campaign_Brief.docx'.\n",
    "8.  **Final Confirmation:** Output the confirmation message.\n",
    "\n",
    "Begin the process following Step 1.\n",
    "\"\"\")\n",
    "\n",
    "# Recreate the supervisor workflow with the updated prompt\n",
    "supervisor_workflow = create_supervisor(\n",
    "    [summarizer_agent, brief_generator_agent],\n",
    "    model=llm,\n",
    "    tools=tools_for_supervisor,\n",
    "    prompt=supervisor_prompt\n",
    ")\n",
    "\n",
    "# Recompile the app\n",
    "app = supervisor_workflow.compile()\n",
    "\n",
    "# --- REVISED INPUT SECTION ---\n",
    "\n",
    "# Define the core campaign concept (relatively static)\n",
    "core_concept_prompt = \"\"\"\n",
    "Core Campaign Concept: Generate a comprehensive campaign brief focused on promoting our existing **EcoSmart Thermos** product line for the upcoming Fall/Winter season.\n",
    "Target Audience: Environmentally conscious commuters, students, and outdoor enthusiasts who value durability, sustainability, and keeping beverages at the right temperature during colder months.\n",
    "Primary Objectives: 1. Increase sales of the EcoSmart Thermos line by 20% compared to the previous Fall/Winter season. 2. Reinforce brand association with high-quality, sustainable drinkware solutions. 3. Drive traffic to the EcoSmart Thermos product pages on our website.\n",
    "Core Message Basis: Emphasize warmth, reliability, and eco-friendliness - \"Warmth That Lasts, Waste That Doesn't. Choose EcoSmart Thermos This Season.\"\n",
    "Channel Suggestions: Targeted social media (Instagram, Facebook - sustainability focus), eco-conscious bloggers/influencers, segmented email marketing, potential SEM.\n",
    "Required Brief Components: Detailed Campaign Objectives, Specific Target Audience Segments (personas), Finalized Key Message, Proposed Channel Mix with rationale, Ideas for Creative Assets (e.g., cozy lifestyle imagery, videos demonstrating heat retention, graphics highlighting eco-benefits), and Key Performance Indicators (KPIs) to track success (e.g., sales figures, website conversion rate, social media engagement, influencer reach).\n",
    "\"\"\"\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Please provide the specific details for the EcoSmart Thermos Fall/Winter Campaign:\")\n",
    "print(\"---\")\n",
    "\n",
    "# Get dynamic inputs from the user\n",
    "campaign_name_input = input(\"Enter the desired Campaign Name (e.g., EcoSmart Winter Warmth '24): \")\n",
    "duration_input = input(\"Enter the Campaign Duration (e.g., Oct 15, 2024 - Jan 15, 2025): \")\n",
    "budget_input = input(\"Enter the Total Campaign Budget (e.g., $25,000): \")\n",
    "roles_input = input(\"Enter Key Roles/Responsibilities (e.g., Campaign Manager: Jane D.; Creative Lead: Sam R.): \")\n",
    "\n",
    "# Combine core concept and dynamic details into the initial message\n",
    "initial_user_message_content = f\"\"\"\n",
    "User's Request: Create a campaign brief based on the following concept and specific details.\n",
    "\n",
    "--- Core Campaign Concept ---\n",
    "{core_concept_prompt}\n",
    "\n",
    "--- Specific User-Provided Details ---\n",
    "Campaign Name: {campaign_name_input}\n",
    "Campaign Duration: {duration_input}\n",
    "Total Budget: {budget_input}\n",
    "Roles & Responsibilities: {roles_input}\n",
    "--- End of User Request ---\n",
    "\n",
    "Please proceed with generating the brief and populating the template, ensuring all placeholders are filled using both the core concept and these specific details.\n",
    "\"\"\"\n",
    "\n",
    "# Use the combined input in the invoke call\n",
    "print(\"\\n--- Invoking Workflow with Combined Prompt ---\")\n",
    "result = app.invoke({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": initial_user_message_content # Pass combined prompt to supervisor\n",
    "            }\n",
    "        ]\n",
    "    },{\"recursion_limit\": 100}) # Set appropriate recursion limit\n",
    "\n",
    "# Print the final results\n",
    "print(\"\\n--- Workflow Final Output ---\")\n",
    "for m in result['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09c288-d94c-40a8-82d0-069fba504c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
